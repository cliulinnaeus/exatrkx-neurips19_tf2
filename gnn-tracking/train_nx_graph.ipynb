{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Training GNN\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "prog_name = os.path.basename(sys.argv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ipykernel_launcher.py] save models at ./out/segments_100/v0_kaggle\n",
      "[ipykernel_launcher.py] last iteration: 0\n",
      "Total Events: 100 with 16 sections, total 1600 files \n",
      "Training data: [0, 79] events, total 1280 files\n",
      "Testing data:  [80, 100] events, total 320 files\n",
      "Training and testing graphs are selected sequantially from their corresponding pools\n",
      "Maximum iterations per job: 80000\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import re\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import sonnet as snt\n",
    "\n",
    "from graph_nets import utils_tf\n",
    "from graph_nets import utils_np\n",
    "\n",
    "from heptrkx.nx_graph import utils_train\n",
    "from heptrkx.nx_graph import prepare\n",
    "from heptrkx.nx_graph import get_model\n",
    "from heptrkx import load_yaml\n",
    "from heptrkx.nx_graph.utils_io import ckpt_name\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='Train nx-graph with configurations')\n",
    "# add_arg = parser.add_argument\n",
    "# add_arg('config',  nargs='?', default='configs/data_5000evts.yaml')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "all_config = load_yaml('./configs/train_edge_classifier_kaggle_share.yaml')\n",
    "config = all_config['segment_training']\n",
    "\n",
    "\n",
    "# add ops to save and restore all the variables\n",
    "prod_name = config['prod_name']\n",
    "output_dir = os.path.join(config['output_dir'], prod_name)\n",
    "print(\"[{}] save models at {}\".format(prog_name, output_dir))\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "files = glob.glob(output_dir+\"/*.ckpt.meta\")\n",
    "last_iteration = 0 if len(files) < 1 else max([\n",
    "    int(re.search('checkpoint_([0-9]*).ckpt.meta', os.path.basename(x)).group(1))\n",
    "    for x in files\n",
    "])\n",
    "# print(files)\n",
    "# print(\"jajdsjfsaifjsaof\")\n",
    "# print(len(files))\n",
    "print(\"[{}] last iteration: {}\".format(prog_name, last_iteration))\n",
    "\n",
    "# default 2/3 for training and 1/3 for testing\n",
    "input_nxgraphs_dir = all_config['make_graph']['out_graph']\n",
    "generate_input_target = prepare.inputs_generator(input_nxgraphs_dir, n_train_fraction=0.8)\n",
    "\n",
    "config_tr = config['parameters']\n",
    "# How much time between logging and printing the current results.\n",
    "# save checkpoint very 10 mins\n",
    "log_every_seconds       = config_tr['time_lapse']\n",
    "batch_size = n_graphs   = config_tr['batch_size']   # need optimization\n",
    "num_training_iterations = config_tr['iterations']\n",
    "iter_per_job            = 2500 if 'iter_per_job' not in config_tr else config_tr['iter_per_job']\n",
    "num_processing_steps_tr = config_tr['n_iters']      ## level of message-passing\n",
    "print(\"Maximum iterations per job: {}\".format(iter_per_job))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:\n",
    "@tf.function()\n",
    "def get_data(n_graphs, is_trained=True):\n",
    "    inputs, targets = generate_input_target(n_graphs, is_trained)\n",
    "    if isinstance(inputs[0], dict):\n",
    "        input_graphs  = utils_np.data_dicts_to_graphs_tuple(inputs)\n",
    "        target_graphs = utils_np.data_dicts_to_graphs_tuple(targets)\n",
    "    else:\n",
    "        input_graphs  = utils_np.networkxs_to_graphs_tuple(inputs)\n",
    "        target_graphs = utils_np.networkxs_to_graphs_tuple(targets)\n",
    "    return input_graphs, target_graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize input_graphs and target_graphs as instances of GraphsTuple\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(config['model_name'])\n",
    "input_graphs, target_graphs = get_data(n_graphs)\n",
    "# dir(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, trainable=False)\n",
    "start_learning_rate = config_tr['learning_rate'] # 0.001\n",
    "learning_rate = tf.compat.v1.train.exponential_decay(\n",
    "    start_learning_rate, global_step,\n",
    "    decay_steps=500,\n",
    "    decay_rate=0.97, staircase=True)\n",
    "\n",
    "# using a constant learning rate instead:\n",
    "# snt Adam doesn't seem to support decaying learning rate\n",
    "# use tf.keras.optimizers.adam for decaying learning rate\n",
    "optimizer = snt.optimizers.Adam(1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss_weights matrix\n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_weights = 1.0\n",
    "if config_tr['real_weight']:\n",
    "    real_weight = config_tr['real_weight']\n",
    "    fake_weight = config_tr['fake_weight']\n",
    "    loss_weights = target_graphs.edges * real_weight + (1 - target_graphs.edges)*fake_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training step:\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input_data, example_target_data = get_data(n_graphs)\n",
    "input_signature = [\n",
    "    utils_tf.specs_from_graphs_tuple(example_input_data),\n",
    "    utils_tf.specs_from_graphs_tuple(example_target_data),\n",
    "    tf.TensorSpec.from_tensor(loss_weights)\n",
    "]\n",
    "\n",
    "\n",
    "@tf.function(input_signature = input_signature)\n",
    "def update_step(inputs_tr, targets_tr, loss_weights):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs_tr = model._build(inputs_tr, num_processing_steps_tr)\n",
    "#         print(outputs_tr)\n",
    "#         print('ccc')\n",
    "        # Loss:\n",
    "        losses_tr = utils_train.create_loss_ops(targets_tr, outputs_tr, loss_weights)\n",
    "        loss_tr = sum(losses_tr) / num_processing_steps_tr\n",
    "    gradients = tape.gradient(loss_tr, model.trainable_variables)\n",
    "#     print('aaa')\n",
    "    optimizer.apply(gradients, model.trainable_variables)\n",
    "#     print('ddd')\n",
    "    return outputs_tr, loss_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling update_step with tf_function to speed up code:\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input_data, example_target_data = get_data(n_graphs)\n",
    "input_signature = [\n",
    "    utils_tf.specs_from_graphs_tuple(example_input_data),\n",
    "    utils_tf.specs_from_graphs_tuple(example_target_data),\n",
    "    tf.TensorSpec.from_tensor(loss_weights)\n",
    "]\n",
    "\n",
    "# # use this function instead of update_step:\n",
    "# compiled_update_step = tf.function(update_step, input_signature=input_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'double' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ce8b719d8329>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdouble_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'double' is not defined"
     ]
    }
   ],
   "source": [
    "# double_strings = double.get_concrete_function(tf.TensorSpec(shape=None, dtype=tf.string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restore Previous Checkpoint\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading checkpoint: ./out/segments_100/v0_kaggle/checkpoint_00000.ckpt\n"
     ]
    }
   ],
   "source": [
    "# both optimizer and model are snt version, they should be trackable\n",
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, model=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, os.path.join(output_dir, ckpt_name.format(last_iteration)), max_to_keep=3)\n",
    "\n",
    "ckpt.restore(manager.latest_checkpoint)\n",
    "if manager.latest_checkpoint:\n",
    "    print(\"loading checkpoint:\", os.path.join(output_dir, ckpt_name.format(last_iteration)))\n",
    "else:\n",
    "    print(\"Initializing from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./out/segments_100/v0_kaggle'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize\n",
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "logged_iterations = []\n",
    "losses_tr = []\n",
    "corrects_tr = []\n",
    "solveds_tr = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log Training Process\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_str  = time.strftime('%d %b %Y %H:%M:%S', time.localtime())\n",
    "out_str += '\\n'\n",
    "out_str += \"# (iteration number), T (elapsed seconds), Ltr (training loss), Precision, Recall\\n\"\n",
    "log_name = os.path.join(output_dir, config['log_name'])\n",
    "with open(log_name, 'a') as f:\n",
    "    f.write(out_str)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of using feed_dict, manually take the values out\n",
    "inputs_tr, targets_tr = get_data(batch_size)\n",
    "# print(inputs_tr)\n",
    "outputs_tr, loss_tr = update_step(inputs_tr, targets_tr, loss_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Training Steps:\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# (iteration number), TD (get graph), TR (TF run)\n",
      "# 00220, TD 0.0, TR 74.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f0f27ec27101>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# added this: not sure if correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m#     print('aaa')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0moutputs_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;31m#     print('bbb')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mrun_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrun_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-7e5b3075549c>\u001b[0m in \u001b[0;36mupdate_step\u001b[0;34m(inputs_tr, targets_tr, loss_weights)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mlosses_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_loss_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses_tr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_processing_steps_tr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m#     print('aaa')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/upgrade/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/upgrade/lib/python3.7/site-packages/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/upgrade/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/upgrade/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_UnsortedSegmentSumGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_UnsortedSegmentSumGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m   \u001b[0;34m\"\"\"Gradient for UnsortedSegmentSum.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_GatherDropNegatives\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "last_log_time = start_time\n",
    "\n",
    "## loop over iterations, each iteration generating a batch of data for training\n",
    "iruns = 0\n",
    "all_run_time = start_time\n",
    "all_data_taking_time = start_time\n",
    "\n",
    "\n",
    "tot_start_time = start_time\n",
    "iter_time_lst = []\n",
    "\n",
    "# read total time\n",
    "tot_time_already_elapsed = 0\n",
    "with open('./total_time.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        tot_time_already_elapsed = float(line[:-1])\n",
    "\n",
    "        \n",
    "with open('./iter_time.txt', 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        # remove linebreak which is the last character of the string\n",
    "        item = line[:-1]\n",
    "        # add item to the list\n",
    "        iter_time_lst.append(float(item))\n",
    "\n",
    "\n",
    "print(\"# (iteration number), TD (get graph), TR (TF run)\")\n",
    "for iteration in range(last_iteration, num_training_iterations):\n",
    "    iter_start_time = time.time()\n",
    "    if iruns > iter_per_job:\n",
    "        print(\"runs larger than {} iterations per job, stop\".format(iter_per_job))\n",
    "        break\n",
    "    else: iruns += 1\n",
    "    last_iteration = iteration\n",
    "    data_start_time = time.time()\n",
    "    \n",
    "    \n",
    "    # instead of using feed_dict, manually take the values out\n",
    "    inputs_tr, targets_tr = get_data(batch_size)\n",
    "    loss_weights = targets_tr.edges * real_weight + (1 - targets_tr.edges)*fake_weight\n",
    "    all_data_taking_time += time.time() - data_start_time\n",
    "\n",
    "    # timing the run time only \n",
    "    run_start_time = time.time()\n",
    "\n",
    "    # added this: not sure if correct\n",
    "#     print('aaa')\n",
    "    outputs_tr, loss_tr = update_step(inputs_tr, targets_tr, loss_weights)\n",
    "#     print('bbb')\n",
    "    run_time = time.time() - run_start_time\n",
    "    all_run_time += run_time\n",
    "\n",
    "    the_time = time.time()\n",
    "    elapsed_since_last_log = the_time - last_log_time\n",
    "    \n",
    "    iter_time_elapsed = time.time() - iter_start_time\n",
    "    iter_time_lst.append(iter_time_elapsed)\n",
    "   \n",
    "    tot_time_already_elapsed = (time.time() - tot_start_time) + tot_time_already_elapsed\n",
    "   \n",
    "    \n",
    "    if elapsed_since_last_log > log_every_seconds:\n",
    "        # save a checkpoint\n",
    "#         print('in here')\n",
    "        last_log_time = the_time\n",
    "        \n",
    "        inputs_ge, targets_ge = get_data(batch_size, is_trained=False)\n",
    "        loss_weights = targets_ge.edges * real_weight + (1 - targets_ge.edges)*fake_weight\n",
    "        \n",
    "        outputs_ge = model._build(inputs_ge, num_processing_steps_tr)\n",
    "\n",
    "        losses_ge = utils_train.create_loss_ops(targets_ge, outputs_ge, loss_weights)\n",
    "        loss_ge = sum(losses_ge) / num_processing_steps_tr\n",
    "        \n",
    "\n",
    "\n",
    "        correct_tr, solved_tr = utils_train.compute_matrics(\n",
    "            targets_ge, outputs_ge[-1])\n",
    "        elapsed = time.time() - start_time\n",
    "        losses_tr.append(loss_tr)\n",
    "        corrects_tr.append(correct_tr)\n",
    "        solveds_tr.append(solved_tr)\n",
    "        logged_iterations.append(iteration)\n",
    "        out_str = \"# {:05d}, T {:.1f}, Ltr {:.4f}, Lge {:.4f}, Precision {:.4f}, Recall {:.4f}\\n\".format(\n",
    "            iteration, elapsed, loss_tr, loss_ge,\n",
    "            correct_tr, solved_tr)\n",
    "\n",
    "        run_cost_time = all_run_time - start_time\n",
    "        data_cost_time = all_data_taking_time - start_time\n",
    "        print(\"# {:05d}, TD {:.1f}, TR {:.1f}\".format(iteration, data_cost_time, run_cost_time))\n",
    "        with open(log_name, 'a') as f:\n",
    "            f.write(out_str)\n",
    "        \n",
    "        ckpt.step.assign_add(1)\n",
    "        save_path = manager.save()\n",
    "        \n",
    "        \n",
    "        # save current elapsed time:\n",
    "        with open('./total_time.txt', 'w') as f:\n",
    "            f.write(str(tot_time_already_elapsed))\n",
    "        \n",
    "        # save iter time list:\n",
    "        with open('./iter_time.txt', 'w') as filehandle:\n",
    "            for listitem in iter_time_lst:\n",
    "                filehandle.write('{}\\n'.format(str(listitem)))\n",
    "        \n",
    "        \n",
    "        # if precision and recall are both very good, log total time and break\n",
    "        if correct_tr >= 0.95 and solved_tr >= 0.95:\n",
    "            tot_time_elapsed = (time.time() - tot_start_time) + tot_time_already_elapsed\n",
    "            mean_iter_time_elapsed = np.mean(iter_time_lst)\n",
    "            with open(log_name, 'a') as f:\n",
    "                f.write(\"final precision: {}, final recall: {}\".format(correct_tr, solved_tr))\n",
    "                f.write(\"total time elapsed: {}\".format(tot_time_elapsed))\n",
    "                f.write(\"average iteration time elapsed: {}\".format(mean_iter_time_elapsed))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-upgrade]",
   "language": "python",
   "name": "conda-env-.conda-upgrade-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
