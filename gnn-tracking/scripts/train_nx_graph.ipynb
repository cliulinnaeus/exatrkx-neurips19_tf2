{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Training GNN\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "prog_name = os.path.basename(sys.argv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ipykernel_launcher.py] save models at ../out/segments_100/v0_kaggle\n",
      "[ipykernel_launcher.py] last iteration: 0\n",
      "Total Events: 100 with 16 sections, total 1600 files \n",
      "Training data: [0, 79] events, total 1280 files\n",
      "Testing data:  [80, 100] events, total 320 files\n",
      "Training and testing graphs are selected sequantially from their corresponding pools\n",
      "Maximum iterations per job: 80000\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import re\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import sonnet as snt\n",
    "\n",
    "from graph_nets import utils_tf\n",
    "from graph_nets import utils_np\n",
    "\n",
    "from heptrkx.nx_graph import utils_train\n",
    "from heptrkx.nx_graph import prepare\n",
    "from heptrkx.nx_graph import get_model\n",
    "from heptrkx import load_yaml\n",
    "from heptrkx.nx_graph.utils_io import ckpt_name\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='Train nx-graph with configurations')\n",
    "# add_arg = parser.add_argument\n",
    "# add_arg('config',  nargs='?', default='configs/data_5000evts.yaml')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "all_config = load_yaml('../configs/train_edge_classifier_kaggle_share.yaml')\n",
    "config = all_config['segment_training']\n",
    "\n",
    "\n",
    "# add ops to save and restore all the variables\n",
    "prod_name = config['prod_name']\n",
    "output_dir = os.path.join(config['output_dir'], prod_name)\n",
    "print(\"[{}] save models at {}\".format(prog_name, output_dir))\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "files = glob.glob(output_dir+\"/*.ckpt.meta\")\n",
    "last_iteration = 0 if len(files) < 1 else max([\n",
    "    int(re.search('checkpoint_([0-9]*).ckpt.meta', os.path.basename(x)).group(1))\n",
    "    for x in files\n",
    "])\n",
    "print(\"[{}] last iteration: {}\".format(prog_name, last_iteration))\n",
    "\n",
    "# default 2/3 for training and 1/3 for testing\n",
    "input_nxgraphs_dir = all_config['make_graph']['out_graph']\n",
    "generate_input_target = prepare.inputs_generator(input_nxgraphs_dir, n_train_fraction=0.8)\n",
    "\n",
    "config_tr = config['parameters']\n",
    "# How much time between logging and printing the current results.\n",
    "# save checkpoint very 10 mins\n",
    "log_every_seconds       = config_tr['time_lapse']\n",
    "batch_size = n_graphs   = config_tr['batch_size']   # need optimization\n",
    "num_training_iterations = config_tr['iterations']\n",
    "iter_per_job            = 2500 if 'iter_per_job' not in config_tr else config_tr['iter_per_job']\n",
    "num_processing_steps_tr = config_tr['n_iters']      ## level of message-passing\n",
    "print(\"Maximum iterations per job: {}\".format(iter_per_job))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:\n",
    "@tf.function\n",
    "def get_data(n_graphs, is_trained=True):\n",
    "    inputs, targets = generate_input_target(n_graphs, is_trained)\n",
    "    if isinstance(inputs[0], dict):\n",
    "        input_graphs  = utils_np.data_dicts_to_graphs_tuple(inputs)\n",
    "        target_graphs = utils_np.data_dicts_to_graphs_tuple(targets)\n",
    "    else:\n",
    "        input_graphs  = utils_np.networkxs_to_graphs_tuple(inputs)\n",
    "        target_graphs = utils_np.networkxs_to_graphs_tuple(targets)\n",
    "    return input_graphs, target_graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize input_graphs and target_graphs as instances of GraphsTuple\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(config['model_name'])\n",
    "input_graphs, target_graphs = get_data(n_graphs)\n",
    "# dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "# isinstance(SegmentClassifier, model)\n",
    "print('hi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, trainable=False)\n",
    "start_learning_rate = config_tr['learning_rate'] # 0.001\n",
    "learning_rate = tf.compat.v1.train.exponential_decay(\n",
    "    start_learning_rate, global_step,\n",
    "    decay_steps=500,\n",
    "    decay_rate=0.97, staircase=True)\n",
    "\n",
    "# using a constant learning rate instead:\n",
    "# snt Adam doesn't seem to support decaying learning rate\n",
    "# use tf.keras.optimizers.adam for decaying learning rate\n",
    "optimizer = snt.optimizers.Adam(1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss_weights matrix\n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_weights = 1.0\n",
    "if config_tr['real_weight']:\n",
    "    real_weight = config_tr['real_weight']\n",
    "    fake_weight = config_tr['fake_weight']\n",
    "    loss_weights = target_graphs.edges * real_weight + (1 - target_graphs.edges)*fake_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training step:\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_step(inputs_tr, targets_tr, loss_weights):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs_tr = model._build(inputs_tr, num_processing_steps_tr)\n",
    "#         print(outputs_tr)\n",
    "#         print('ccc')\n",
    "        # Loss:\n",
    "        losses_tr = utils_train.create_loss_ops(targets_tr, outputs_tr, loss_weights)\n",
    "        loss_tr = sum(losses_tr) / num_processing_steps_tr\n",
    "    gradients = tape.gradient(loss_tr, model.trainable_variables)\n",
    "#     print('aaa')\n",
    "    optimizer.apply(gradients, model.trainable_variables)\n",
    "#     print('ddd')\n",
    "    return outputs_tr, loss_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling update_step with tf_function to speed up code:\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input_data, example_target_data = get_data(n_graphs)\n",
    "input_signature = [\n",
    "    utils_tf.specs_from_graphs_tuple(example_input_data),\n",
    "    utils_tf.specs_from_graphs_tuple(example_target_data),\n",
    "    tf.TensorSpec.from_tensor(loss_weights)\n",
    "]\n",
    "\n",
    "# # use this function instead of update_step:\n",
    "compiled_update_step = tf.function(update_step, input_signature=input_signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restore Previous Checkpoint\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if last_iteration > 0:\n",
    "    print(\"loading checkpoint:\", os.path.join(output_dir, ckpt_name.format(last_iteration)))\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(output_dir, ckpt_name.format(last_iteration)),\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize\n",
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logged_iterations = []\n",
    "losses_tr = []\n",
    "corrects_tr = []\n",
    "solveds_tr = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log Training Process\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_str  = time.strftime('%d %b %Y %H:%M:%S', time.localtime())\n",
    "out_str += '\\n'\n",
    "out_str += \"# (iteration number), T (elapsed seconds), Ltr (training loss), Precision, Recall\\n\"\n",
    "log_name = os.path.join(output_dir, config['log_name'])\n",
    "with open(log_name, 'a') as f:\n",
    "    f.write(out_str)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(112130, 1), dtype=float64, numpy=\n",
       "array([[0.5],\n",
       "       [0.5],\n",
       "       [0.5],\n",
       "       ...,\n",
       "       [0.5],\n",
       "       [0.5],\n",
       "       [0.5]])>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of using feed_dict, manually take the values out\n",
    "inputs_tr, targets_tr = get_data(batch_size)\n",
    "# print(inputs_tr)\n",
    "outputs_tr, loss_tr = update_step(inputs_tr, targets_tr, loss_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.36492985, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(loss_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Training Steps:\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# (iteration number), TD (get graph), TR (TF run)\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "in here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/c/cheliu/.conda/envs/upgrade/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 00004, TD 0.0, TR 74.3\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "in here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/c/cheliu/.conda/envs/upgrade/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 00009, TD 0.0, TR 142.2\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "in here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/c/cheliu/.conda/envs/upgrade/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 00014, TD 0.0, TR 209.3\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "in here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/c/cheliu/.conda/envs/upgrade/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 00019, TD 0.0, TR 276.0\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "in here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/c/cheliu/.conda/envs/upgrade/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 00024, TD 0.1, TR 341.9\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "in here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/c/cheliu/.conda/envs/upgrade/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 00029, TD 0.1, TR 405.3\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "in here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/c/cheliu/.conda/envs/upgrade/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 00034, TD 0.1, TR 470.1\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "in here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/c/cheliu/.conda/envs/upgrade/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 00038, TD 0.1, TR 526.4\n",
      "aaa\n",
      "bbb\n",
      "aaa\n",
      "bbb\n",
      "aaa\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "last_log_time = start_time\n",
    "\n",
    "## loop over iterations, each iteration generating a batch of data for training\n",
    "iruns = 0\n",
    "all_run_time = start_time\n",
    "all_data_taking_time = start_time\n",
    "\n",
    "print(\"# (iteration number), TD (get graph), TR (TF run)\")\n",
    "for iteration in range(last_iteration, num_training_iterations):\n",
    "    if iruns > iter_per_job:\n",
    "        print(\"runs larger than {} iterations per job, stop\".format(iter_per_job))\n",
    "        break\n",
    "    else: iruns += 1\n",
    "    last_iteration = iteration\n",
    "    data_start_time = time.time()\n",
    "    \n",
    "    \n",
    "    # instead of using feed_dict, manually take the values out\n",
    "    inputs_tr, targets_tr = get_data(batch_size)\n",
    "    loss_weights = targets_tr.edges * real_weight + (1 - targets_tr.edges)*fake_weight\n",
    "    all_data_taking_time += time.time() - data_start_time\n",
    "\n",
    "    # timing the run time only \n",
    "    run_start_time = time.time()\n",
    "\n",
    "    # added this: not sure if correct\n",
    "    print('aaa')\n",
    "    outputs_tr, loss_tr = update_step(inputs_tr, targets_tr, loss_weights)\n",
    "    print('bbb')\n",
    "    run_time = time.time() - run_start_time\n",
    "    all_run_time += run_time\n",
    "\n",
    "    the_time = time.time()\n",
    "    elapsed_since_last_log = the_time - last_log_time\n",
    "\n",
    "    if elapsed_since_last_log > log_every_seconds:\n",
    "        # save a checkpoint\n",
    "        print('in here')\n",
    "        last_log_time = the_time\n",
    "        \n",
    "        inputs_ge, targets_ge = get_data(batch_size, is_trained=False)\n",
    "        loss_weights = targets_ge.edges * real_weight + (1 - targets_ge.edges)*fake_weight\n",
    "        \n",
    "        outputs_ge = model._build(inputs_ge, num_processing_steps_tr)\n",
    "\n",
    "        losses_ge = utils_train.create_loss_ops(targets_ge, outputs_ge, loss_weights)\n",
    "        loss_ge = sum(losses_ge) / num_processing_steps_tr\n",
    "        \n",
    "\n",
    "\n",
    "        correct_tr, solved_tr = utils_train.compute_matrics(\n",
    "            targets_ge, outputs_ge[-1])\n",
    "        elapsed = time.time() - start_time\n",
    "        losses_tr.append(loss_tr)\n",
    "        corrects_tr.append(correct_tr)\n",
    "        solveds_tr.append(solved_tr)\n",
    "        logged_iterations.append(iteration)\n",
    "        out_str = \"# {:05d}, T {:.1f}, Ltr {:.4f}, Lge {:.4f}, Precision {:.4f}, Recall {:.4f}\\n\".format(\n",
    "            iteration, elapsed, loss_tr, loss_ge,\n",
    "            correct_tr, solved_tr)\n",
    "\n",
    "        run_cost_time = all_run_time - start_time\n",
    "        data_cost_time = all_data_taking_time - start_time\n",
    "        print(\"# {:05d}, TD {:.1f}, TR {:.1f}\".format(iteration, data_cost_time, run_cost_time))\n",
    "        with open(log_name, 'a') as f:\n",
    "            f.write(out_str)\n",
    "        \n",
    "        # not sure how to save\n",
    "#         save_path = saver.save(\n",
    "#             sess,\n",
    "#             os.path.join(output_dir, ckpt_name.format(iteration)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-upgrade]",
   "language": "python",
   "name": "conda-env-.conda-upgrade-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
